<html>
<head>
<style>
  body {
    margin-top: 30px;
    margin-bottom: 30px;
    margin-left: 50px;
    margin-right: 50px;
  }
  a {text-decoration : none; color : #003399;}
  a:hover {text-decoration:underline; color: #8C1515; }

  .disabled{
      pointer-events: none;
      color:black;
  }
  .left {
    width: 200px;
  }
</style>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109673810-1');
</script>


<title>Mengqing Jiang | Carnegie Mellon University</title>
<link rel="icon" href="images/nlp_logo.jpg">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Mengqing Jiang">
<meta name="keywords" content="Mengqing Jiang, Tsinghua, Berkeley, Mengqing, Carnegie Mellon University, CMU, Robotics Institute">
<meta name="robots" content="all">
<meta name="description" content="Homepage of Mengqing Jiang">
</head>

<body>

<table cellpadding="20" style="font-size: 17px">
<tbody>
  <tr>
    <td><a href="images/mengqing.jpg"><img src="images/mengqing.jpeg" alt="Mengqing Jiang" ,="" height="220"></a></td>
    <td>
      <h1> Mengqing Jiang </h1>
      <font size="+1"><b>Master of Science, Computer Vision </b> <br> <br>
        <b>Robotics Institute</b> <br>
      <b>School of Computer Science</b> <br><b>Carnegie Mellon University</b> <br> <br>
      <b>Email</b>: <tt>jiangmengqing1121 AT gmail.com</tt> <br>
      <!-- <b>Office</b>: Gates 234 --></font> <br><br>
      <a href="#publications">[Publications]</a> <a href="#projects">[Projects]</a> 
      <a href="#education">[Education]</a> <a href="#experience">[Experience]</a> <br>
      <a href="#awards">[Awards]</a> <a href="#misc">[Miscellaneous]</a> <br>
    </td>   
  </tr>
</tbody></table>

<p>Hi! I am currently a first-year graduate student at Carnegie Mellon University</a>. I obtained my B.Eng. in Software Engineering from Tsinghua University. During my undergrad study, I took a research internship at <a href="https://deepdrive.berkeley.edu/">Berkeley Deep Drive</a> advised by <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>.
Before that, I have worked with <a href="https://www.cse.ust.hk/~hunkim/">Prof. Sung Kim</a> remotely for a year and visited HKUST twice. I also had a great time interning at <a href="https://www.momenta.ai/en/">Momenta</a> on autonomous driving, and <a href="https://www.sensetime.com/">SenseTime Inc.</a> on computer vision.</p>

<p>My interests lie in computer vision, robotics and their applications. I will be interning in Uber ATG's perception team for Summer 2019.</p> 

<!-- <center><b>Looking for 2019 summer internship! <a href="res/jmqCV.pdf">Download my CV</a> </b></center> -->
<b><a href="res/jmqCV.pdf">Download my CV</a> </b>


<a name="projects" class="disabled"><h2> Projects </h2></a>
<table>
<tbody>
<tr><td></td><td style="text-align:left"><b>MPilot algorithm simulation on UE4 with CarSim plugins</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="200" src="images/projects/mpilot.gif" alt="mpilot"> </td>
  <td valign="top"><b>Mengqing Jiang</b>, Zizhe Xu, Sibo Jia<br> 
  Mar 2018 - Jun 2018, Momenta <br>  
  <em>In order to conduct large-scale automated safety tests for the auto-pilot algorithms of Momenta, called MPilot, I developed a pipeline for Lincoln MKZ dynamics simulation on Unreal Engine 4 with CarSim plugins, reproduced the algorithm in the simulator, and created testing scenarios, such as the front car sharply decelerate and left/right side car cut in. Moreover, I also investigated active sub-lane changing algorithms for autonomous car in the case of traffic congestion and did experiments in this simulator.</em> <!-- <br> <font size="-1">[pdf] [video] </font>  --><br><br> <br></td>
</tr>

<tr><td></td><td><br></td></tr>
<tr><td></td><td style="text-align:left"><b>Imitated Control for Vehicle Pedestrian Interaction</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="200" src="images/projects/interaction.gif" alt="interaction"> </td>
  <td valign="top"><b>Mengqing Jiang</b>, Nathan Lambert, Fisher Yu, Anca Dragan, Trevor Darrell <br> 
  July 2017- Nov 2017, UC Berkeley <br>  
  <em>In this work, we present a regression model acting as a vehicle controller for the interaction between a vehicle approaching and a pedestrian crossing an intersection. Vehicle and human are detected by clustering the LIDAR point cloud. Using a LSTM network, the vehicle predicts the desired velocity even through mis-identification of point-cloud data. This prediction model demonstrates the use of LSTM for spotty data and future trajectory planning applications.</em> <!-- <br> <font size="-1">[pdf] [video] </font>  --><br><br> <br></td>
</tr>

<tr><td></td><td><br></td></tr>

<tr><td></td><td style="text-align:left"><b>Experimental Platform And Visualization Dashboard on ROS for Self-driving Car</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="200" src="images/projects/dashboard.png" alt="dashboard"> </td>
  <td valign="top"><b>Mengqing Jiang</b>, Gray Chen, Yujia Luo, Fisher Yu <br> 
  July 2017- Oct 2017, UC Berkeley <br>  
  <em>To provide a better platform for conducting and debugging experiments on the unmanned vehicle, we implement a bunch of visualization tools and integrate them into one dashboard. The tools can view camera images (including original ones and object detection bounding boxes), check the control signal values and plot charts, visualize the LIDAR point cloud, plot driving track on Google maps.according to the GPS information. Moreover, we add timeline for rosbag player so that the messages in bag files can be easily rewinded and checked.</em> <br> <font size="-1"><a href="https://github.com/Jmq14/ROS_car_dashboard">[code]</a></font> </td>
</tr>

<tr><td></td><td><br></td></tr>

<tr><td></td><td style="text-align:left"><b>Doodle2Code</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="200" src="images/projects/doodle2code.png" alt="doodle2code"> </td>
  <td valign="top"><b>Mengqing Jiang</b>, Xiaodong Gu, Sung Kim, Chunping Li  <br> 
  Sep 2016-Aug 2017, Tsinghua University, Hong Kong University of Science and Technology <br>  
  <em>It has been a typical and essential task to transforming a hand-written doodle into HTML/CSS in order to build customized websites and arrange their layouts, since it is easier and faster for designers to illustrate ideas drawing on canvas, which is also very challenging. In this work, we use recent advances in Image Caption and represent an end-to-end deep neural model with a CNN-encoder and an RNN-decoder, translating a web page layout into HTML code that displays as the given image after browser rendering. Accordingly, two datasets are built for this task: a large-scale program-generated web screenshot dataset and a collected hand-written web doodle dataset. The proposed model performs well on both datasets and outputs code with high quality and high accuracy, achieving 66.45 and 49.21 BLEU score respectively with no HTML grammar errors. In addition, experiments have proved that transfer learning from the large-scale screenshot dataset strongly enhances the model’s performance on the doodle dataset. Moreover, web application named "Doodle2Code" has been developed, allowing users to translate their doodles into HTML code and modify the generated code until seeing the exact web page they wanted online, on which a user study is conducted.</em> <br> <font size="-1"><a href="res/projects/theis.pdf">[thesis (in Chinese)]</a><!-- [website<i>(coming soon!)</i>] --></font>  </td>
</tr>

<tr><td></td><td><br></td></tr>

<tr><td></td><td style="text-align:left"><b>Residual Attention Network for Image Classification</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="200" src="images/projects/attention.png" alt="attention"> </td>
  <td valign="top">Fei Wang, <b>Mengqing Jiang</b>, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang <br> 
  Aug 2016-Nov 2016, SenseTime Inc. <br>  
  <em>In this work, we propose “Residual Attention Network”, a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion, which is built by stacking Attention Modules generating attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.</em> <br> <font size="-1">[<a href="https://arxiv.org/pdf/1704.06904.pdf">paper</a>][<a href="https://github.com/fwang91/residual-attention-network">code</a>] </font>  </td>
</tr>

<tr><td></td><td><br></td></tr>

<tr><td></td><td style="text-align:left"><b>WeLearn (WeChat App)</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="120" src="images/projects/welearn.png" alt="welearn"> </td>
  <td valign="top">Zhaoyang Li, Yonghe Wang, <b>Mengqing Jiang</b>, Bin Liu  <br> 
  Nov 2016-Dec 2016, Tsinghua University <br>  
  <em>We implemented a mobile web application based on WeChat (most popular SNS application in China). After timely crawling course assignments, announcements, etc. from Tsinghua WebLearning Website, WeLearn provides user-friendly interface, including dashboard, calendar and so, for students to check the homework, announcements, lectures and other information, as well as sends notifications to alert important deadlines. Beside, we integrated the "Team Finder" function into WeLearn for students to find project group members online. </em> <br> <font size="-1">[<a href="http://jmq14.github.io/2016/12/30/welearn/">blog(in Chinese)</a>][<a href="https://github.com/lizy14/weixuetang">code</a>] </font>  </td>
</tr>

<tr><td></td><td><br></td></tr>

<tr><td></td><td style="text-align:left"><b>On The Road (Webpage Game)</b></td></tr>
<tr>
  <td width="210" style="text-align:center"><img width="120" src="images/projects/ontheroad.gif" alt="ontheroad"> </td>
  <td valign="top"><b>Mengqing Jiang</b>, Yonghe Wang <br> 
  May 2016, Tsinghua University <br>  
  <em>We developed a low-poly style web game based on a javascript 3D engine called Three.js. The goal of this game is to moving the little red square on the path as far as possible. By tapping space bar, the square will make a turn, so the timing is the key to this game. Enjoy the game!</em> <br> <font size="-1">[<a href="http://jmq14.github.io/2016/07/16/WebGame/">blog(in Chinese)</a>][<a href="https://github.com/Cyclops-THSS/On-the-Road">code</a>] [<a href="http://jmq14.github.io/OnTheRoad/index.html">demo</a>] </font>  </td>
</tr>
</tbody>
</table>

<a name="publications" class="disabled"><h2> Publications </h2></a>
<ul>
  <li> <b>Residual Attention Network for Image Classification</b><br>
  Fei Wang, <b>Mengqing Jiang</b>, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang <br>
  CVPR 2017 <i><font color=CornflowerBlue >(spotlight)</font> </i></li>
  <font size="-1"> [<a href="https://arxiv.org/pdf/1704.06904.pdf">paper</a>][<a href="https://github.com/fwang91/residual-attention-network">code</a>] </font><br><br>

  <li> 
    <b>Scalable Discrete Supervised Multimedia Hash Learning with Clustering</b><br> 
    Shifeng Zhang, Jianmin Li, <b>Mengqing Jiang</b>, Bo Zhang <br> 
    IEEE TCSVT 2017 </li>
  <font size="-1">[<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7937842">paper</a>]</font>
  <br>
</ul>


<a name="education" class="disabled"><h2> Education </h2></a>
<ul>
<li> 2018.8 - 2019.12(expected): Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
  <br>Master of Science, Computer Vision
  <br><b>Current QPA</b>: 4.22/4.33
  <br> Courses: Introduction to Machine Learning; Computer Vision; Math Fundamentals for Robotics; Visual Learning and Recognition (in progress); Robot Localization and Mapping (in progress)</li>
  <br>
<li> 2014.9 - 2018.7: School of Software, Tsinghua University, Beijing, China
  <br>B.Eng. in Software Engineering
  <br><b>Overall GPA</b>: 90/100 (3.9/4.0), <b>Rank Top 10%</b></li>
  <br>
</ul>

<a name="experience" class="disabled"><h2> Experience </h2></a>
<ul>
<li> Mar 2018 - Jun 2018: Engineer intern at Momenta (Auto-pilot algorithms and products group), Beijing, China.
  <br> Working with Sibo jia, ZiZhe Xu</li>
  <br>
<li> Jun 2017 - Nov 2017: Research intern at Berkeley DeepDrive, UC Berkeley, CA, USA.  <br> 
   Supervised by Prof. Trevor Darrell, Dr. Fisher Yu</li>
   <br>
<li> May 2016 - Nov 2016: Research intern at Sensetime Inc. (Computer vision research group), Beijing, China
  <br> Working with Chen Qian, Fei Wang</li>
</ul>

<a name="awards" class="disabled"><h2> Selected Awards </h2></a>
<ul>
  <li> Graduation with distinction, Tsinghua University, 2018</li>
  <li> Science and Technology Innovation Scholarship, 2017</li>
  <li> National Scholarship, 2016</li>
  <li> Qualcomm STEM Scholarship, 2016 </li>
  <li> WeTech Qualcomm Global Scholarship, 2016</li>
  <li> Dong’s Scholarship, Tsinghua University, 2015</li>
  <li> Outstanding Student Leadership Award, Tsinghua University, 2015</li>
  <li> Second Prize in National Olympiad in Informatics in Provinces (NOIP), 2011</li>
</ul>


<a name="misc" class="disabled"><h2> Miscellaneous </h2></a>
<ul>
  <li> <a href="http://tts.imtranslator.net/axG1">How to pronounce my name?</a> In the Wade-Giles system of romanization, it is rendered as meng-ch'ing chiang. <br>In Chinese characters, it is 蒋梦青. </li>
  <li> I love reading and writing. Check my <a href="http://jmq14.github.io">Blog</a> (in Chinese) where my ideas and life were recorded.</li>
  
</ul>
Last updated: 2019/01.

<blockquote style="text-align:center"><i>
All boundaries are conventions, waiting to be transcended. <br>
One may transcend any convention if only one can first conceive of doing so. <br>
― David Mitchell, Cloud Atlas</i>
</blockquote>
</html>
